# PP 2024 hw4
## Implementation
### 1. Describe how you implemented the FlashAttention forward pass using CUDA. Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (‚Ñì and ùëö) were calculated.
The main difference between the algorithm provided in the spec and the actual cuda implementaion is that blocks in the inner loop are paralleled through gpu blocks and the calculation is done by threads. The Tr and Batch for-loop remain the same. I will illustrate the details below.
1. **Matrix Blocking**:
   - Matrices \( Q \), \( K \), and \( V \) are divided into smaller blocks to allow efficient computation within the GPU's limited shared memory.
   - Rows are divided by br, col are divided by bc. However, in my implementation, I set both to 32 to fully utilize threads per block.

2. **Shared Memory Usage (SRAM)**:
   - The total share memory I used are `kj`, `vj`, `sij`, `pij`, `qi`, `oi`, `li`, `mi`, `mij`, `lij`, these are the memory specified in the algorithm. Plus, the intermidiate variable in `UpdateMiLiOi()`, `mi_new`, `li_new`. I store them because it's array, it will become global mem if not using share memory.
   - The total size is below the limit **49152** by deviceQuery. Some sizes are 32, 32*32, 32*64. Total usage is 32\*64\*4+32\*32\*2+32\*6 = 10432 amount of float. 10432\*4 = 41728 bytes. 

3. **Computation of Scaling Factors**:
   - \( m \): The maximum value for each row of the scaled dot-product \( QK^T \) is computed to stabilize the softmax calculation.
   - \( l \): The row sum of exponentiated values is calculated after subtracting \( m \) for numerical stability.
---

### 2. How are matrices \( Q \), \( K \), and \( V \) divided into blocks and processed in parallel?
- **Block Division**:
  - \( Q \): Divided into blocks of size \( Br * d \), where \( Br \) is the number of rows per block, 32.
  - \( K \): Divided into blocks of size \( d * Bc \), where \( Bc \) is the number of columns per block, 32.
  - \( V \): Divided into blocks of size \( Bc * d \).

- **Parallel Processing**:
  - Each CUDA block processes one block of \( Qi \), iterating over corresponding blocks of \( Kj \) and \( Vj \).
  - Threads within a block collaboratively compute the scaled dot-product \( QK^T \), apply softmax, and update the output matrix \( O \) block by block.
  - Outter loop(Kj, Vj) cannot be paralleled since there is dependency issue.

---

### 3. How were the block sizes \( Br \) and \( Bc \) chosen, and why?

To fully utilize threads per block (1024). I set them to be 32x32. The higher threads per block we use, the more we utilize the pbsm since we load global memory to pbsm less.
Also, d is guarantee to be {32, 64}, so 32 is ideal for simplicity.

---

### 4. Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.

- **Threads Per Block**:
  - \( 1024 \) threads per block, arranged in a \( 32 * 32 \) grid corresponding to \( Br \) and \( Bc \).

- **Grid Dimensions**:
  - The grid dimensions are derived from the size of \( Q \) and the blocking factors:
    - Grid.x = \( N / Br \): The number of row blocks in \( Q \).
    - Grid.y = \( N / Bc \): The number of column blocks in \( K \).
- SRAM is declare inside kernel since it's fixed. The size of them is detailed above.

---

### 5. Justify your choices and how they relate to the blocking factors and the SRAM size.
### 5. Justify Your Choices and How They Relate to the Blocking Factors and the SRAM Size

The chosen blocking factors (\( Br = 32 \), \( Bc = 32 \)) maximize thread utilization (1024 threads per block) while aligning with \( d \) values (\( 32, 64 \)) for simplicity. This ensures efficient parallelization of inner loop computations and minimizes global memory access by caching critical data in shared memory.

The shared memory allocation (\( 41728 \) bytes) is within the device's limit (\( 49152 \) bytes), enabling optimal caching for \( kj, vj, sij, pij \), and scaling factors. This design reduces memory latency and supports stable, iterative updates required for softmax computation.

## Profiling Result
### Environment Spec 
* Apollo GPU
* I use t22 as testcase since it's the slowest testcase 
* Profile Command
```bash=
srun -p nvidia -N1 -n1 --gres=gpu:1 nvprof --metrics gld_throughput ./hw4 /home/pp24/share/hw4/testcases/t22 t22.out
```
| Metric                          | Avg Value          |
|---------------------------------|----------------|
| **Occupancy**                   | 0.4999      |
| **SM Efficiency**               | 98.47%         |
| **Shared Memory Load Throughput** | 3558 GB/s    |
| **Shared Memory Store Throughput** | 107 GB/s    |
| **Global Load Throughput**      | 11.103 GB/s    |
| **Global Store Throughput**     | 13.5 GB/s    |

## Experiment & Analysis
### Environment Spec 
* Apollo GPU
* I use t22 as testcase since it's the slowest

### Optimization

* Share memory
    * Implement shared memory according to the flashattention algorithm
* Unroll
    * Use pragma unroll every for loop
* Handle bank conflict
    * Use padding, the original share memory array size is [32*64]. Since 64 is multiple of 32, I pad it  to [32*65].
    * Below table shows the bank conflict before and after, I use nvprof with shared_ld(st)_bank_conflict to get the matrix.

|  | before  | after |
| -------- | -------- | -------- |
| #bank conflict     |     1.5032e+10  | 0    |

* One time Memory Allocate
    * The original algorithm approach is in each batch, we allocate N*d Q,K,V......etc. This involve B times of memory allocation and copy, which is time-consuming. I allocate B*N*d Q,K,V......etc once to reduce the overhead.
* kernel fusion
    * Originally I write each qkdot, rowmax...... as a single kernel function. But I found out there might be kernel launching overhead, so I fuse all of them.
* Remove redundant syncthreads
    * syncthreads() function is crucial in correctness. However, adding too much will cause huge performance degradation. I try to remove redundant calls, which improve performance a lot.

![imgae](https://i.imgur.com/5dhVM9l.png)
* Figure Analysis
    * I use t22 as testcase since it has the poorest performance
    * I use shared Memory version as the baseline.
    * we can see Unroll actually brings barely no optimization, I guess the compiler already do the job for me.
    * Bank conflict, one-time memory allocation and kernel fusion brings significant improvement.
    * remove redundant syncthreads only optimize a little bit.
### Other 
#### Do we really need m[i]?
Inside the attention algorithm, m[i] is used to handle overflow in exponential instruction. However, I found the even the largest case will not cause exponential overflow. So we can actually remvoe the m[i] in this assignment, which significantly reduce memory allocatio and copy, calculation instructions.
#### Parallizing Batches
In the sample code provided by TA, data is batched and call flashattention sequentially. In the original approach, I use N/bc block to parallel tc. However, we can also parallel batches. 
Finally, I use (N/bc, B) dimension of blocks to parallel.
The total performance improved about 10%.




### Experience and conclusion
This is an interesting homework. I learned a lot. Flashattention is important and powerful in LLM computation, thanks Ta for providing this meaningful assignment.